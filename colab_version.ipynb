{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab_version.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OIgg2vCJ6rXd","colab_type":"text"},"source":["This code is for training a neural network into learning how to play the atari game from the gym API.\n","\n","The process will consist on loading multiple datasets that must be in the working directory of this notebook, concatenating them to create a dataset and parsing the dataset to the network.\n","\n","The user is free to change the number of epochs and the parameters of the neural network. However, I do not advise to change the parameters of network since the training accuracy is most likely to reduce considerably. The most sensitive parameter is the loss. All the possible options of loss were tried and none of them gave a result as good as binary_crossentropy."]},{"cell_type":"code","metadata":{"id":"-a4XaMf--rrw","colab_type":"code","outputId":"fad857ba-68cc-4228-c594-bc7dfe86d4c3","executionInfo":{"status":"ok","timestamp":1589038486129,"user_tz":-120,"elapsed":2948,"user":{"displayName":"Juan Álvarez","photoUrl":"","userId":"06378331534551784227"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","import gym\n","import cv2\n","import argparse\n","import sys, glob\n","import numpy as np\n","import pandas as pd\n","import pdb\n","import keras\n","from keras import backend as K\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers import Input, Dense, Reshape\n","from keras.layers.wrappers import TimeDistributed\n","from keras.optimizers import Adam, Adamax, RMSprop\n","from keras.layers.advanced_activations import PReLU\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.core import Activation, Dropout, Flatten\n","from keras.layers.convolutional import UpSampling2D, Convolution2D\n","IMAGE_LENGTH = int(33600)\n","epochs = 1\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oieGtLiX-vUM","colab_type":"code","colab":{}},"source":["# This code is for loading the images of the dataset given a path.\n","# IMPORTANT: The dataset most contain flattenned image of leng 33600 and\n","# before the first element of this vector there must be the feature of that\n","# sample. That feature is the keyboard pressed by the user while playing and \n","# it has to be an integer between 0 and 8.\n","def load_data(path):\n","    #pdb.set_trace()\n","    df = pd.read_csv(path, sep = '.', header = None)\n","    letters = df[(df.index % (IMAGE_LENGTH+1)==0)].values.tolist()\n","    images = df[(df.index % (IMAGE_LENGTH+1)!=0)].values.tolist()\n","    n =IMAGE_LENGTH\n","    final = [images[i * n:(i + 1) * n] for i in range((len(images) + n - 1) // n )]  \n","    #pdb.set_trace()\n","    return letters, final"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lE1zpo3e-6Ky","colab_type":"code","colab":{}},"source":["# Prepare first dataset \n","letter, final = load_data('/content/drive/My Drive/DATOS/EIT/NICE ACADEMIC/AI/project/dataset_thomas_v1.txt')\n","for i in range (0, len(final)):\n","    final[i] = np.concatenate(final[i])\n","#pdb.set_trace()\n","\n","letter = np.concatenate(letter)\n","final = np.array(final)\n","letter=letter.reshape(1, len(letter))\n","letter = letter[0, :]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8AFl_qkGDZEY","colab_type":"code","colab":{}},"source":["# Prepare  second dataset \n","letter2, final2 = load_data('/content/drive/My Drive/DATOS/EIT/NICE ACADEMIC/AI/project/dataset_thomas_v2.txt')\n","for i in range (0, len(final2)):\n","    final2[i] = np.concatenate(final2[i])\n","#pdb.set_trace()\n","\n","letter2 = np.concatenate(letter2)\n","final2 = np.array(final2)\n","letter2=letter2.reshape(1, len(letter2))\n","letter2 = letter2[0, :]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_OT1w5HFdmG","colab_type":"code","colab":{}},"source":["# Prepare third dataset\n","# Prepare  second dataset \n","letter3, final3 = load_data('/content/drive/My Drive/DATOS/EIT/NICE ACADEMIC/AI/project/dataset_thomas_v3.txt')\n","for i in range (0, len(final3)):\n","    final3[i] = np.concatenate(final3[i])\n","#pdb.set_trace()\n","\n","letter3 = np.concatenate(letter3)\n","final3 = np.array(final3)\n","letter3=letter3.reshape(1, len(letter3))\n","letter3 = letter3[0, :]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUmt25YMM0io","colab_type":"code","colab":{}},"source":["# Prepare third dataset\n","# Prepare  second dataset \n","letter4, final4 = load_data('/content/drive/My Drive/DATOS/EIT/NICE ACADEMIC/AI/project/dataset_haider_v1.txt')\n","for i in range (0, len(final4)):\n","    final4[i] = np.concatenate(final4[i])\n","#pdb.set_trace()\n","\n","letter4 = np.concatenate(letter4)\n","final4 = np.array(final4)\n","letter4=letter4.reshape(1, len(letter4))\n","letter4 = letter4[0, :]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eg2lgyvJEdu7","colab_type":"code","colab":{}},"source":["final = np.concatenate((final, final2, final3, final4))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TLZS1JvE7bs","colab_type":"code","colab":{}},"source":["letter = np.concatenate((letter, letter2, letter3, letter4))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9gxAZo5FFru","colab_type":"code","outputId":"91b690cd-6622-4f60-e36e-62a0df76e5f4","executionInfo":{"status":"ok","timestamp":1589039138407,"user_tz":-120,"elapsed":652901,"user":{"displayName":"Juan Álvarez","photoUrl":"","userId":"06378331534551784227"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# This is printing the number of images we have of gameplay. \n","# This number should be much higher to properly train the network however 13 thousand\n","# already correspond to above 30 minutes of game play.\n","print('We have a total of {} frames ready for training!'.format(final.shape[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["We have a total of 13233 frames ready for training!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9cSMMhGGEEpt","colab_type":"code","colab":{}},"source":["# Split dataset using a rule of 0.7\n","train_ratio = 0.7\n","\n","n_train_samples = int(len(final) * train_ratio)\n","x_train, y_train = final[:n_train_samples], letter[:n_train_samples]\n","x_val, y_val = final[n_train_samples:], letter[n_train_samples:]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPf0vS9I-8sc","colab_type":"code","colab":{}},"source":["# The number of classes are all the possible keys a user can press during the game\n","# Even not pressing any key is allowed.\n","num_classes = 9\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_val = keras.utils.to_categorical(y_val, num_classes)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6w5FwMQCDDsn","colab_type":"code","outputId":"0964de50-9436-405f-e31e-2dcbfb98f021","executionInfo":{"status":"ok","timestamp":1589039430645,"user_tz":-120,"elapsed":5902,"user":{"displayName":"Juan Álvarez","photoUrl":"","userId":"06378331534551784227"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Input shape will be 33600 since that is the image size.\n","# This neural network has only 2 layers. The user is free to try new layers.\n","# An experiment with eleven layers was performed but no improvement was observed.\n","model = Sequential()\n","model.add(Dense(150, activation='tanh', input_shape=(33600,)))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='nadam',\n","              metrics=['accuracy'])\n","#pdb.set_trace()\n","history = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_val, y_val))\n","score = model.evaluate(x_val, y_val, verbose=0)\n","print('Validation loss:', score[0])\n","print('Validation accuracy:', score[1])\n","# This code saves the model which will be needed for letting the network play the game\n","model_json = model.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","model.save_weights(\"model_weights.h5\")\n","print(\"Saved model to disk\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 9263 samples, validate on 3970 samples\n","Epoch 1/1\n","9263/9263 [==============================] - 4s 426us/step - loss: 0.1885 - accuracy: 0.9201 - val_loss: 0.6235 - val_accuracy: 0.8205\n","Validation loss: 0.6235493505631646\n","Validation accuracy: 0.8205432891845703\n","Saved model to disk\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lVo9vJ-ZcT_i","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"vaxp92FLcfyo","colab_type":"code","outputId":"bb455d23-53d9-4735-dc9c-b9562140493b","executionInfo":{"status":"ok","timestamp":1589921275892,"user_tz":-120,"elapsed":3363,"user":{"displayName":"Juan Álvarez","photoUrl":"","userId":"06378331534551784227"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Based on the excellent\n","# https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n","# and uses Keras.\n","import os\n","import pdb\n","import gym\n","import cv2\n","import argparse\n","import sys, glob\n","import numpy as np\n","from keras import backend as K\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers import Input, Dense, Reshape\n","from keras.layers.wrappers import TimeDistributed\n","from keras.optimizers import Adam, Adamax, RMSprop\n","from keras.layers.advanced_activations import PReLU\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.core import Activation, Dropout, Flatten\n","from keras.layers.convolutional import UpSampling2D, Conv2D"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"yBi6Z6nMcVHN","colab_type":"code","colab":{}},"source":["#Script Parameters\n","input_dim = 210 * 160\n","gamma = 0.99\n","update_frequency = 1\n","learning_rate = 0.001\n","resume = False\n","render = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBhmdnyXcnRr","colab_type":"code","colab":{}},"source":["#Initialize\n","env = gym.make('Enduro-v0')\n","#pdb.set_trace()\n","number_of_inputs = env.action_space.n #This is incorrect for Pong (?)\n","#number_of_inputs = 1\n","observation = env.reset()\n","prev_x = None\n","xs, dlogps, drs, probs = [],[],[],[]\n","running_reward = None\n","reward_sum = 0\n","episode_number = 0\n","train_X = []\n","train_y = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dzmlGuwpcpb9","colab_type":"code","colab":{}},"source":["def pong_preprocess_screen(I):\n","  I=np.dot(I[..., :3], [0.2989, 0.5870, 0.1140])\n","  #pdb.set_trace()\n","  return I.astype(np.float).ravel()\n","def discount_rewards(r):\n","  discounted_r = np.zeros_like(r)\n","  running_add = 0\n","  for t in reversed(range(0, r.size)):\n","    if r[t] != 0: running_add = 0\n","    running_add = running_add * gamma + r[t]\n","    discounted_r[t] = running_add\n","  return discounted_r\n","\n","#Define the main model (WIP)\n","def learning_model(input_dim = input_dim, model_type=1):\n","  model = Sequential()\n","  if model_type==0:\n","    model.add(Reshape((1,210,160), input_shape=(input_dim,)))\n","    model.add(Flatten())\n","    model.add(Dense(200, activation = 'relu'))\n","    model.add(Dense(number_of_inputs, activation='softmax'))\n","    opt = RMSprop(lr=learning_rate)\n","  else:\n","    model.add(Reshape((1,210,160), input_shape=(input_dim,)))\n","    model.add(Conv2D(32, (9, 9), activation=\"relu\", strides=(4, 4), padding=\"same\", kernel_initializer=\"he_uniform\"))\n","    model.add(Flatten())\n","    model.add(Dense(16, activation=\"relu\", kernel_initializer=\"he_uniform\"))\n","    model.add(Dense(number_of_inputs, activation='softmax'))\n","    opt = Adam(lr=learning_rate)\n","  model.compile(loss='categorical_crossentropy', optimizer=opt)\n","  if resume == True:\n","    model.load_weights('atari_model_checkpoint.h5')\n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ukvc9HW7cvYZ","colab_type":"code","colab":{}},"source":["model = learning_model()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHk2kZYCcyVv","colab_type":"code","outputId":"9f2938bb-c838-42c6-e5a3-7f5313405dd4","executionInfo":{"status":"error","timestamp":1589926323500,"user_tz":-120,"elapsed":836845,"user":{"displayName":"Juan Álvarez","photoUrl":"","userId":"06378331534551784227"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Begin training\n","while True:\n","  if render: \n","    env.render()\n","  #Preprocess, consider the frame difference as features\n","  cur_x = pong_preprocess_screen(observation)\n","  #pdb.set_trace()\n","  x = cur_x - prev_x if prev_x is not None else np.zeros(input_dim)\n","  prev_x = cur_x\n","  #Predict probabilities from the Keras model\n","  aprob = ((model.predict(x.reshape([1,x.shape[0]]), batch_size=1).flatten()))\n","  #aprob = aprob/np.sum(aprob)\n","  #Sample action\n","  #action = np.random.choice(number_of_inputs, 1, p=aprob)\n","  #Append features and labels for the episode-batch\n","  xs.append(x)\n","  probs.append((model.predict(x.reshape([1,x.shape[0]]), batch_size=1).flatten()))\n","  aprob = aprob/np.sum(aprob)\n","  action = np.random.choice(number_of_inputs, 1, p=aprob)[0]\n","  y = np.zeros([number_of_inputs])\n","  y[action] = 1\n","  #print action\n","  dlogps.append(np.array(y).astype('float32') - aprob)\n","  observation, reward, done, info = env.step(action)\n","  reward_sum += reward\n","  drs.append(reward) \n","  if done:\n","    #pdb.set_trace()\n","    episode_number += 1\n","    # Puts as a matrix the photos\n","    epx = np.vstack(xs)\n","    epdlogp = np.vstack(dlogps)\n","    epr = np.vstack(drs)\n","    discounted_epr = discount_rewards(epr)\n","    mean = np.mean(discounted_epr)\n","    std = np.std(discounted_epr) if np.std(discounted_epr) > 0 else 1\n","    discounted_epr = (discounted_epr-mean)/std\n","    epdlogp *= discounted_epr\n","    #Slowly prepare the training batch\n","    train_X.append(xs) \n","    train_y.append(epdlogp)\n","    xs,dlogps,drs = [],[],[]\n","    #Periodically update the model\n","    if episode_number % update_frequency == 0: \n","      y_train = probs + learning_rate * np.squeeze(np.vstack(train_y)) #Hacky WIP\n","      #y_train[y_train<0] = 0\n","      #y_train[y_train>1] = 1\n","      #y_train = y_train / np.sum(np.abs(y_train), axis=1, keepdims=True)\n","      print ('Training Snapshot:')\n","      #pdb.set_trace()\n","      print (y_train)\n","      model.train_on_batch(np.squeeze(np.vstack(train_X)), y_train)\n","      #Clear the batch\n","      train_X = []\n","      train_y = []\n","      probs = []\n","      #Save a checkpoint of the model\n","      os.remove('atari_model_checkpoint.h5') if os.path.exists('atari_model_checkpoint.h5') else None\n","      model.save_weights('atari_model_checkpoint.h5')\n","    #Reset the current environment nad print the current results\n","    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n","    print('Environment reset imminent. Total Episode Reward: {}. Running Mean: {}'.format(reward_sum, running_reward))\n","    reward_sum = 0\n","    observation = env.reset()\n","    prev_x = None\n","  if reward != 0:\n","    print(('Episode {} \\n '.format(episode_number)))\n","    if reward == -1:\n","        print('Defeat')\n","    else:\n","        print('VICTORY!')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training Snapshot:\n","[[1.11111112e-01 1.11111112e-01 1.11111112e-01 ... 1.11111112e-01\n","  1.11111112e-01 1.11111112e-01]\n"," [1.05619133e-02 4.12464171e-04 1.74888261e-02 ... 9.08532739e-01\n","  3.76470659e-08 8.31604877e-04]\n"," [3.19967367e-04 4.29355123e-06 8.62146087e-04 ... 9.38069522e-01\n","  7.04219332e-04 5.63389971e-04]\n"," ...\n"," [2.64926784e-05 1.80292773e-05 2.89394520e-02 ... 1.03733875e-02\n","  5.42349136e-03 8.94116893e-05]\n"," [8.71038139e-02 9.23015978e-06 3.55539260e-06 ... 4.82413918e-01\n","  1.66023136e-04 5.04706707e-03]\n"," [3.86817845e-07 1.61061858e-11 4.17597583e-08 ... 9.85582709e-01\n","  1.98356375e-07 3.93617083e-05]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.1111106e-01 1.1111106e-01 1.1111109e-01 ... 1.1111121e-01\n","  1.1111117e-01 1.1111110e-01]\n"," [1.0492862e-02 4.0883818e-04 1.7401118e-02 ... 9.0880764e-01\n","  3.7388567e-08 8.2685269e-04]\n"," [2.0325261e-04 2.7156866e-06 1.0205087e-03 ... 9.9242097e-01\n","  1.1633394e-05 1.3451895e-04]\n"," ...\n"," [5.7860065e-01 1.5204422e-03 1.6205515e-01 ... 2.2553992e-01\n","  9.6257143e-05 1.4103303e-03]\n"," [3.9080484e-04 5.1459560e-06 2.6450152e-04 ... 7.0480490e-01\n","  3.6270554e-05 4.7076220e-04]\n"," [5.2203340e-03 1.4806018e-04 1.1855873e-02 ... 9.3330896e-01\n","  4.9330667e-04 5.4541356e-03]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11111030e-01 1.11110993e-01 1.11111052e-01 ... 1.11111395e-01\n","  1.11111239e-01 1.11111104e-01]\n"," [9.42503195e-03 2.54330167e-04 1.49481855e-02 ... 9.20811176e-01\n","  2.81909234e-08 7.19031843e-04]\n"," [1.42185609e-05 3.43101867e-08 2.51198944e-05 ... 9.99358237e-01\n","  3.30351650e-05 4.08629130e-05]\n"," ...\n"," [4.38863069e-01 1.90596955e-04 2.65192360e-01 ... 1.51416403e-04\n","  2.85024657e-06 1.95882239e-06]\n"," [1.12209727e-06 5.42864700e-06 1.13866510e-04 ... 9.81212676e-01\n","  9.45878960e-03 6.37520687e-04]\n"," [1.98877454e-02 7.67994879e-06 1.12220924e-03 ... 9.48995411e-01\n","  4.85903380e-04 1.10997055e-02]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11110985e-01 1.11110911e-01 1.11111075e-01 ... 1.11111604e-01\n","  1.11111239e-01 1.11111104e-01]\n"," [1.48405144e-02 5.33980317e-04 1.62201431e-02 ... 8.97510529e-01\n","  4.18009058e-08 1.08328008e-03]\n"," [1.26576185e-01 4.09332775e-02 7.79025853e-02 ... 2.58163542e-01\n","  8.28425959e-02 1.42611772e-01]\n"," ...\n"," [4.62775677e-01 2.19762675e-03 3.84233845e-09 ... 1.46299208e-04\n","  6.00775024e-07 3.10182646e-02]\n"," [4.49231738e-05 1.52644744e-10 2.11188035e-06 ... 3.11478972e-01\n","  2.70656155e-08 2.25518491e-07]\n"," [2.99301967e-02 3.85340827e-04 1.91939380e-05 ... 8.32653418e-02\n","  2.96921849e-06 7.46268988e-01]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11111023e-01 1.11110903e-01 1.11111067e-01 ... 1.11111619e-01\n","  1.11111157e-01 1.11111112e-01]\n"," [9.31942184e-03 2.50964978e-04 1.49638690e-02 ... 9.21441078e-01\n","  2.80338455e-08 7.13999267e-04]\n"," [4.01698938e-03 7.65724908e-05 6.43639883e-04 ... 2.27942660e-01\n","  1.00987474e-03 2.62627704e-03]\n"," ...\n"," [2.59115836e-06 6.13624707e-06 2.53085932e-03 ... 9.92399395e-01\n","  7.89066945e-08 2.66994955e-03]\n"," [8.94136906e-01 1.07330989e-08 1.12810255e-07 ... 1.01656765e-01\n","  2.71119552e-06 4.65139647e-06]\n"," [5.42940339e-04 1.13698654e-04 6.96684653e-03 ... 7.95767546e-01\n","  9.62908939e-02 6.21146671e-02]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11111075e-01 1.11110948e-01 1.11111052e-01 ... 1.11111559e-01\n","  1.11111112e-01 1.11111119e-01]\n"," [9.37856454e-03 2.52380356e-04 1.49986083e-02 ... 9.21202064e-01\n","  2.80999046e-08 7.17094517e-04]\n"," [1.49923846e-01 8.04969668e-03 1.18955985e-01 ... 1.80520136e-02\n","  4.72091623e-02 8.40851455e-04]\n"," ...\n"," [1.02352402e-04 3.20084860e-06 5.17488434e-06 ... 2.13885820e-03\n","  3.54833514e-06 4.06392837e-06]\n"," [1.42625722e-04 1.37376117e-06 1.30420522e-05 ... 6.23870611e-01\n","  2.34294498e-06 2.69101904e-04]\n"," [1.04851075e-04 5.30690585e-08 1.39674824e-03 ... 4.54581855e-03\n","  5.86595654e-01 5.86609531e-05]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11111157e-01 1.11110978e-01 1.11110948e-01 ... 1.11111462e-01\n","  1.11110948e-01 1.11111127e-01]\n"," [1.50840571e-02 5.41267800e-04 1.63111016e-02 ... 8.96688044e-01\n","  4.19225330e-08 1.09377166e-03]\n"," [1.26622677e-01 4.09464240e-02 7.79339075e-02 ... 2.58122146e-01\n","  8.28366503e-02 1.42623216e-01]\n"," ...\n"," [1.00958545e-07 1.25522892e-09 1.97566636e-02 ... 9.60410237e-01\n","  2.78735030e-07 6.11228461e-06]\n"," [1.43330663e-01 3.72550062e-07 6.04884053e-06 ... 7.36212790e-01\n","  1.14057237e-07 2.40036286e-04]\n"," [2.42319966e-05 7.53976201e-06 6.14737580e-03 ... 2.70237611e-03\n","  1.15724233e-05 7.08800826e-07]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11111134e-01 1.11110859e-01 1.11110754e-01 ... 1.11111663e-01\n","  1.11110799e-01 1.11111104e-01]\n"," [9.51442868e-03 2.55657069e-04 1.49602778e-02 ... 9.20341790e-01\n","  2.80582970e-08 7.20725744e-04]\n"," [4.68789181e-03 8.57237101e-05 1.15151610e-03 ... 3.26292306e-01\n","  1.70281518e-03 3.23370611e-03]\n"," ...\n"," [6.72065094e-03 4.87703176e-07 8.36405743e-05 ... 8.43959272e-01\n","  1.31689023e-06 3.86534703e-06]\n"," [2.55239083e-08 1.13375090e-05 9.96546090e-01 ... 8.78235078e-05\n","  3.88396347e-06 8.13671249e-07]\n"," [3.06167058e-03 6.85020450e-06 3.77419019e-05 ... 8.48044515e-01\n","  1.06434461e-04 6.30112132e-03]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11111097e-01 1.11110650e-01 1.11110553e-01 ... 1.11112170e-01\n","  1.11110456e-01 1.11111075e-01]\n"," [9.47644282e-03 2.54357816e-04 1.48332892e-02 ... 9.20215189e-01\n","  2.78582668e-08 7.17987248e-04]\n"," [3.98615375e-03 7.57218586e-05 6.12213626e-04 ... 2.23297268e-01\n","  9.82305617e-04 2.62272009e-03]\n"," ...\n"," [4.47594941e-01 2.69314717e-03 8.16882700e-02 ... 2.20945835e-01\n","  4.74647060e-03 2.40119323e-02]\n"," [1.15337380e-05 3.25474448e-16 2.38623482e-10 ... 9.99972820e-01\n","  2.00808259e-09 8.77512036e-07]\n"," [2.00758010e-01 2.00385321e-03 5.47367890e-05 ... 2.11302321e-02\n","  4.65641357e-02 3.88904065e-02]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11111104e-01 1.11110397e-01 1.11110598e-01 ... 1.11112736e-01\n","  1.11109987e-01 1.11110911e-01]\n"," [1.51177496e-02 5.40109468e-04 1.60569567e-02 ... 8.95804703e-01\n","  4.14163672e-08 1.08806731e-03]\n"," [1.26681775e-01 4.09272946e-02 7.79215321e-02 ... 2.58251220e-01\n","  8.26936141e-02 1.42607480e-01]\n"," ...\n"," [6.59220442e-02 5.75660379e-05 1.86659358e-02 ... 6.55269742e-01\n","  1.03316689e-02 7.02169200e-04]\n"," [7.11781830e-02 5.20087997e-06 4.40448144e-04 ... 7.42844120e-02\n","  3.95626048e-05 1.22261474e-06]\n"," [1.72296297e-02 3.84292868e-03 6.46473235e-03 ... 8.50289464e-02\n","  6.37924531e-04 4.59929854e-02]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11110911e-01 1.11110166e-01 1.11110710e-01 ... 1.11113377e-01\n","  1.11109443e-01 1.11110739e-01]\n"," [9.45423450e-03 2.53145787e-04 1.46545041e-02 ... 9.19932008e-01\n","  2.75291239e-08 7.12725916e-04]\n"," [3.90127534e-03 7.35986396e-05 5.91393851e-04 ... 2.20616385e-01\n","  9.36962548e-04 2.56477413e-03]\n"," ...\n"," [1.53318506e-05 6.71464946e-08 3.54039344e-06 ... 8.83857012e-01\n","  9.62618068e-02 8.62842353e-05]\n"," [1.61013216e-01 1.02170082e-02 1.86721254e-02 ... 2.05552891e-01\n","  9.79817007e-04 4.80729379e-02]\n"," [2.83676893e-01 8.58162716e-03 2.00455729e-02 ... 2.64960110e-01\n","  9.17632803e-02 3.59771098e-03]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.1111068e-01 1.1110992e-01 1.1111080e-01 ... 1.1111433e-01\n","  1.1110873e-01 1.1111058e-01]\n"," [9.3599819e-03 2.5050042e-04 1.4448867e-02 ... 9.1986966e-01\n","  2.7074137e-08 7.0598396e-04]\n"," [4.4893394e-03 8.0429723e-05 1.0670536e-03 ... 3.1933159e-01\n","  1.5386894e-03 3.0891655e-03]\n"," ...\n"," [1.1841294e-06 2.7093994e-11 1.5638208e-11 ... 7.0432085e-01\n","  4.3887848e-13 3.4766513e-06]\n"," [4.5363337e-02 2.6933412e-04 1.4596060e-03 ... 5.6473172e-01\n","  4.6035384e-05 2.0336643e-02]\n"," [2.6063222e-05 8.1078452e-04 1.3488594e-01 ... 9.5814148e-06\n","  5.3156400e-03 9.9075089e-07]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11110248e-01 1.11109674e-01 1.11111239e-01 ... 1.11115240e-01\n","  1.11107767e-01 1.11110240e-01]\n"," [1.46476878e-02 5.21584763e-04 1.54377194e-02 ... 8.95897985e-01\n","  3.94739388e-08 1.05501025e-03]\n"," [1.26722351e-01 4.08734158e-02 7.79014230e-02 ... 2.58658767e-01\n","  8.23481008e-02 1.42518416e-01]\n"," ...\n"," [2.13364279e-03 2.33149221e-05 3.24929890e-04 ... 3.97695750e-01\n","  1.52213324e-04 5.01141243e-04]\n"," [7.96625465e-02 4.43241447e-02 6.52647242e-02 ... 1.46448091e-01\n","  8.55548307e-02 8.83587748e-02]\n"," [3.00998837e-02 2.79311365e-07 4.42171950e-05 ... 8.36291730e-01\n","  2.46219606e-05 9.47647059e-05]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.1110980e-01 1.1110954e-01 1.1111199e-01 ... 1.1111585e-01\n","  1.1110670e-01 1.1110966e-01]\n"," [1.0054265e-02 3.8488160e-04 1.6171871e-02 ... 9.0829211e-01\n","  3.4270407e-08 7.8549876e-04]\n"," [1.6233489e-05 3.4281531e-08 2.4965701e-05 ... 9.9941325e-01\n","  2.5170415e-05 3.7675014e-05]\n"," ...\n"," [1.4205255e-03 1.4027708e-03 5.1948035e-01 ... 3.7717614e-01\n","  2.6832072e-03 1.2181446e-02]\n"," [8.2194107e-05 4.1928529e-06 5.5655710e-02 ... 7.6599389e-01\n","  8.0433920e-02 1.6724842e-06]\n"," [1.0008710e-01 1.3717720e-03 3.7506790e-05 ... 4.1527014e-02\n","  4.1988649e-04 1.2746704e-02]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.1110943e-01 1.1110924e-01 1.1111267e-01 ... 1.1111662e-01\n","  1.1110573e-01 1.1110915e-01]\n"," [9.0553518e-03 2.3960270e-04 1.3831011e-02 ... 9.1980726e-01\n","  2.5636474e-08 6.8019785e-04]\n"," [1.3078065e-04 4.2195497e-06 1.8235621e-03 ... 9.8520643e-01\n","  2.1382875e-03 2.6092216e-04]\n"," ...\n"," [4.8197694e-06 1.4933636e-14 2.4664897e-09 ... 9.9900776e-01\n","  5.6681781e-08 5.5157830e-08]\n"," [3.7471090e-02 1.9551048e-04 6.1984523e-03 ... 7.2892624e-01\n","  1.0693155e-02 1.8654909e-02]\n"," [4.0883591e-05 8.7160901e-10 4.2918941e-06 ... 9.9775130e-01\n","  1.1825547e-06 1.2398401e-04]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.1110929e-01 1.1110891e-01 1.1111317e-01 ... 1.1111760e-01\n","  1.1110463e-01 1.1110871e-01]\n"," [1.4306521e-02 5.0138996e-04 1.4844329e-02 ... 8.9551628e-01\n","  3.7384620e-08 1.0188464e-03]\n"," [1.2677953e-01 4.0796623e-02 7.7881217e-02 ... 2.5922829e-01\n","  8.1841245e-02 1.4241229e-01]\n"," ...\n"," [5.5983486e-07 9.6659321e-09 1.6490903e-05 ... 9.9886608e-01\n","  1.0911416e-05 8.5439900e-04]\n"," [3.0943122e-02 1.0744302e-02 3.6319733e-01 ... 1.1602481e-01\n","  2.0384241e-02 2.5379123e-02]\n"," [7.1345850e-05 6.5617396e-09 2.0779259e-04 ... 5.3569055e-01\n","  4.5121071e-01 7.7157753e-04]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11109018e-01 1.11108534e-01 1.11113720e-01 ... 1.11118846e-01\n","  1.11103401e-01 1.11108236e-01]\n"," [1.41731091e-02 4.92528197e-04 1.46471420e-02 ... 8.95373464e-01\n","  3.63803530e-08 9.98344505e-04]\n"," [1.46301227e-05 3.84839502e-08 2.99576295e-05 ... 9.99409080e-01\n","  2.65882882e-05 3.56838900e-05]\n"," ...\n"," [4.98069614e-01 1.76930171e-05 1.99514368e-04 ... 1.14183396e-01\n","  8.04846874e-04 4.78436647e-04]\n"," [5.36222826e-04 1.44170059e-07 1.31553898e-04 ... 8.87557149e-01\n","  4.20411561e-06 4.69370186e-03]\n"," [1.57718822e-01 3.53370346e-02 7.10776483e-04 ... 1.91863012e-02\n","  2.72063226e-01 6.16885126e-02]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.1110878e-01 1.1110817e-01 1.1111428e-01 ... 1.1112032e-01\n","  1.1110219e-01 1.1110781e-01]\n"," [9.7769499e-03 3.6336301e-04 1.5465072e-02 ... 9.0774345e-01\n","  3.1452142e-08 7.3731306e-04]\n"," [1.0675088e-01 4.2984419e-02 7.2429985e-02 ... 2.3606057e-01\n","  1.2593660e-01 1.1458052e-01]\n"," ...\n"," [3.1700708e-06 3.6157208e-12 2.6463888e-11 ... 1.3801340e-02\n","  1.9104993e-09 1.4035231e-07]\n"," [2.3302662e-09 1.1789735e-11 4.4208018e-06 ... 9.9486697e-01\n","  3.6505095e-03 1.5225083e-06]\n"," [1.1955293e-06 2.2692369e-12 2.1652085e-09 ... 3.3893637e-04\n","  3.8636672e-12 1.9882178e-10]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11108579e-01 1.11107938e-01 1.11114748e-01 ... 1.11121520e-01\n","  1.11101210e-01 1.11107484e-01]\n"," [8.79906584e-03 2.25255862e-04 1.32703139e-02 ... 9.19341087e-01\n","  2.34547066e-08 6.34840340e-04]\n"," [3.15562985e-03 5.25434953e-05 4.45613085e-04 ... 1.99138701e-01\n","  5.77678613e-04 1.96378026e-03]\n"," ...\n"," [1.71442080e-05 9.98032874e-08 2.37084496e-05 ... 2.43650690e-01\n","  2.57407868e-04 3.83831321e-05]\n"," [5.83735055e-05 1.16816054e-07 2.21704364e-01 ... 1.16849042e-01\n","  4.38833784e-04 2.05511576e-04]\n"," [9.99591291e-01 1.39499343e-06 7.34311755e-07 ... 3.39778780e-04\n","  1.64161875e-08 4.07976495e-06]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11108467e-01 1.11107826e-01 1.11115165e-01 ... 1.11122243e-01\n","  1.11100666e-01 1.11107156e-01]\n"," [9.80577338e-03 3.61273385e-04 1.54786911e-02 ... 9.07040298e-01\n","  3.08057473e-08 7.21419987e-04]\n"," [1.07232394e-04 1.83169698e-06 9.55902738e-04 ... 9.91802990e-01\n","  6.27471483e-04 1.50648309e-04]\n"," ...\n"," [5.00058115e-04 1.56656995e-07 6.08982828e-06 ... 9.64263320e-01\n","  6.15637020e-07 1.59713436e-05]\n"," [1.82549387e-01 1.03945341e-02 1.72703683e-01 ... 7.96193199e-04\n","  4.52089608e-02 8.94667721e-07]\n"," [1.61123407e-05 1.13577553e-05 6.06362522e-03 ... 7.88592160e-01\n","  7.87587924e-05 6.33085205e-04]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11108311e-01 1.11107521e-01 1.11115210e-01 ... 1.11123167e-01\n","  1.11099929e-01 1.11106858e-01]\n"," [8.92904121e-03 2.27970930e-04 1.33430241e-02 ... 9.17831898e-01\n","  2.31060380e-08 6.25897606e-04]\n"," [2.68365955e-04 4.42566125e-06 1.15335023e-03 ... 9.39209640e-01\n","  5.66474162e-04 4.00112476e-04]\n"," ...\n"," [2.69325537e-05 1.76309413e-07 1.05806033e-03 ... 1.21608146e-01\n","  2.07996300e-06 5.20265530e-07]\n"," [1.81588948e-05 1.85144856e-07 7.09358517e-07 ... 2.48687080e-04\n","  1.31671225e-08 1.01534204e-06]\n"," [5.57487772e-04 4.69599009e-08 9.66884254e-04 ... 5.21985115e-03\n","  6.84023917e-01 1.93404339e-04]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.1110845e-01 1.1110730e-01 1.1111541e-01 ... 1.1112392e-01\n","  1.1109909e-01 1.1110656e-01]\n"," [1.4525419e-02 4.9212837e-04 1.4625129e-02 ... 8.9157099e-01\n","  3.4217319e-08 9.4724534e-04]\n"," [1.2686184e-01 4.0671919e-02 7.7816583e-02 ... 2.6023722e-01\n","  8.0977611e-02 1.4223818e-01]\n"," ...\n"," [1.2592625e-02 1.9726451e-04 7.0580752e-03 ... 8.8889964e-02\n","  9.8352393e-05 3.0739183e-04]\n"," [9.6342647e-01 3.0340770e-09 2.7372858e-05 ... 3.4195773e-02\n","  1.1395915e-05 2.2104245e-03]\n"," [1.8230799e-04 2.7660080e-04 8.2080716e-01 ... 6.9139011e-02\n","  3.4318604e-02 3.2973061e-03]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11108936e-01 1.11107141e-01 1.11115552e-01 ... 1.11124009e-01\n","  1.11098282e-01 1.11106224e-01]\n"," [1.49265565e-02 5.03709773e-04 1.48351910e-02 ... 8.89992297e-01\n","  3.42419177e-08 9.48877423e-04]\n"," [1.26910150e-01 4.06744219e-02 7.78123587e-02 ... 2.60267735e-01\n","  8.08907673e-02 1.42224848e-01]\n"," ...\n"," [5.79061329e-01 4.67318430e-04 3.22427113e-05 ... 1.61788594e-02\n","  3.61209004e-07 7.45785597e-04]\n"," [3.62761691e-03 9.87703341e-09 1.06271918e-05 ... 9.91043806e-01\n","  1.33867588e-05 1.31412846e-04]\n"," [8.44725764e-06 3.06800314e-11 9.51207824e-09 ... 9.94252264e-01\n","  3.84116078e-07 8.09847393e-07]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11110680e-01 1.11107066e-01 1.11115508e-01 ... 1.11124314e-01\n","  1.11097425e-01 1.11105926e-01]\n"," [1.53841674e-02 5.15763066e-04 1.51111269e-02 ... 8.88584733e-01\n","  3.43029853e-08 9.51014808e-04]\n"," [3.32724303e-02 3.23664222e-04 5.13337087e-03 ... 1.02691166e-02\n","  8.04629672e-05 2.21906103e-05]\n"," ...\n"," [4.33016510e-07 7.98938249e-09 3.41190165e-03 ... 9.54251051e-01\n","  6.50050609e-08 5.36380242e-07]\n"," [3.34595772e-03 4.41356897e-05 2.16980232e-03 ... 5.68723321e-01\n","  3.12002740e-05 1.08435277e-04]\n"," [8.15290332e-01 6.52247691e-05 4.17624088e-03 ... 1.10415868e-01\n","  8.33749145e-05 5.89518109e-04]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11112982e-01 1.11107007e-01 1.11115165e-01 ... 1.11124486e-01\n","  1.11096382e-01 1.11105628e-01]\n"," [1.59654710e-02 5.31174592e-04 1.53780309e-02 ... 8.86780858e-01\n","  3.44385960e-08 9.57014796e-04]\n"," [1.40403252e-04 2.65055746e-06 1.02031918e-03 ... 9.90784168e-01\n","  9.18228397e-06 1.24767234e-04]\n"," ...\n"," [5.38715860e-03 1.34316160e-05 1.70698215e-03 ... 9.07836378e-01\n","  3.90748307e-03 6.76943511e-02]\n"," [1.79286857e-07 5.57054514e-10 1.73136894e-09 ... 3.64608095e-05\n","  3.14871185e-10 3.43090363e-07]\n"," [3.49494293e-02 1.16941449e-03 3.48220728e-02 ... 5.11732042e-01\n","  3.09048653e-01 7.99523965e-02]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11115679e-01 1.11106798e-01 1.11114465e-01 ... 1.11125223e-01\n","  1.11095309e-01 1.11105524e-01]\n"," [1.65268108e-02 5.43662871e-04 1.55910673e-02 ... 8.85258973e-01\n","  3.44611628e-08 9.60382458e-04]\n"," [1.27172634e-01 4.06645983e-02 7.77513757e-02 ... 2.60443151e-01\n","  8.05940628e-02 1.42272115e-01]\n"," ...\n"," [2.03102468e-06 2.23065186e-07 3.05208191e-03 ... 3.55394930e-02\n","  8.93284321e-01 3.49449001e-05]\n"," [5.47989845e-01 4.01416926e-08 3.08206707e-01 ... 1.43465117e-01\n","  3.49979200e-06 3.51566541e-06]\n"," [4.29788195e-02 2.85177003e-03 2.94906972e-03 ... 7.23413587e-01\n","  1.90272871e-02 1.21120818e-01]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n","Training Snapshot:\n","[[1.11117691e-01 1.11106046e-01 1.11112922e-01 ... 1.11126959e-01\n","  1.11094013e-01 1.11105673e-01]\n"," [1.05504002e-02 2.56801868e-04 1.42503176e-02 ... 9.10832107e-01\n","  2.28010357e-08 6.27452682e-04]\n"," [1.30752400e-01 5.16385213e-03 9.76676792e-02 ... 2.13390756e-02\n","  2.02629808e-02 5.76912949e-04]\n"," ...\n"," [4.26478209e-06 3.71868669e-09 1.55644773e-06 ... 5.93026215e-03\n","  9.78091612e-06 3.14131739e-06]\n"," [9.23265934e-01 1.74572621e-03 1.01814093e-03 ... 7.35801226e-03\n","  4.11756709e-03 4.16847208e-04]\n"," [3.68802100e-02 2.31201513e-04 3.14693823e-02 ... 3.58010978e-01\n","  1.15558160e-02 7.10395572e-04]]\n","Environment reset imminent. Total Episode Reward: 0.0. Running Mean: 0.0\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-d9aff53daf2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m#print action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mdlogps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mdrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ale.lives\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"GJVPV5PhdwBp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nu3b5Nr1_t1Q","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"jPWWxSm2_ubr","colab_type":"code","colab":{}},"source":["from keras.layers import Dense, Activation, Input\n","from keras.models import Model, load_model\n","from keras.optimizers import Adam\n","import keras.backend as K\n","import numpy as np\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNJECBVeAC8R","colab_type":"code","colab":{}},"source":["from keras.layers import Dense, Activation, Input\n","from keras.models import Model, load_model\n","from keras.optimizers import Adam\n","import keras.backend as K\n","import numpy as np\n","import pdb\n","\n","\n","class Agent(object):\n","  def __init__(self, ALPHA, GAMMA = 0.99, n_actions = 9, layer1_size = 16,\n","                 layer2_size = 16, input_dims = 210*160, fname = 'reinforce.h5'):\n","    self.gamma = GAMMA\n","    self.lr = ALPHA\n","    self.G = 0\n","    self.input_dims = input_dims\n","    self.fc1_dims = layer1_size\n","    self.fc2_dims = layer2_size\n","    self.n_actions = n_actions\n","    self.state_memory = []\n","    self.action_memory = []\n","    self.reward_memory = []\n","    self.probabilities = np.zeros(9)\n","\n","    self.policy, self.predict = self.build_policy_network()\n","    self.action_space = [ i for i in range(n_actions)]\n","    self.mode_file = fname\n","\n","  def build_policy_network(self):\n","    input = Input(shape=(self.input_dims,))\n","    advantages = Input(shape=[1])\n","    dense1 = Dense(self.fc1_dims, activation = 'relu')(input)\n","    dense2 = Dense(self.fc2_dims, activation = 'relu')(dense1)\n","    probs = Dense(self.n_actions, activation='softmax')(dense2)\n","\n","    def custom_loss(y_true, y_pred):\n","      out = K.clip(y_pred, 1e-8, 1-1e-8) # To avoid deividing by 0\n","      log_lik = y_true*K.log(out)\n","\n","      return K.sum(-log_lik*advantages)\n","\n","    policy = Model(input=[input, advantages], output = [probs])\n","    policy.compile(optimizer = Adam(lr = self.lr), loss = custom_loss)\n","\n","    predict = Model(input=[input], output = [probs])\n","\n","    return policy, predict\n","\n","  \n","  def choose_action (self, observation):\n","    #pdb.set_trace()\n","    state = observation[np.newaxis, :]\n","    probabilities = self.predict.predict(state)[0]\n","    self.probabilities = np.concatenate((probabilities, self.probabilities), axis = 0)\n","    action = np.random.choice(self.action_space, p = probabilities)\n","    return action\n","\n","  def store_observation(self, observation, action, reward):\n","    self.action_memory.append(action)\n","    self.state_memory.append(observation)\n","    self.reward_memory.append(reward)\n","\n","  def learn(self):\n","    #pdb.set_trace()\n","    state_memory = np.array(self.state_memory)\n","    action_memory = np.array(self.action_memory)\n","    reward_memory = np.array(self.reward_memory)\n","\n","    actions = np.zeros([len(action_memory), self.n_actions])\n","    actions[np.arange(len(action_memory)), action_memory] = 1\n","\n","    G = np.zeros_like(reward_memory)\n","    for t in range(len(reward_memory)):\n","      G_sum = 0\n","      discount = 1\n","      for k in range (t, len(reward_memory)):\n","        G_sum += reward_memory[k]*discount\n","        discount *=self.gamma\n","\n","      G[t] = G_sum\n","    mean = np.mean(G)\n","    std = np.std(G) if np.std(G)>0 else 1\n","    self.G = (G-mean)/std\n","\n","    cost = self.policy.train_on_batch([state_memory, self.G], actions) # y_pred y_true\n","\n","    self.state_memory=[]\n","    self.action_memory = []\n","    self.reward_memory = []\n","\n","    return cost\n","\n","  def save_model(self):\n","    self.policy.save(self.model_file)\n","\n","  def load_model(self):\n","    self.policy = load_model(self.model_file)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"52nwscbGHZen","colab_type":"code","outputId":"5fc98acc-5b71-4c29-cdda-deeb68295f2e","executionInfo":{"status":"error","timestamp":1589928773563,"user_tz":-120,"elapsed":1386,"user":{"displayName":"Juan Álvarez","photoUrl":"","userId":"06378331534551784227"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["def pong_preprocess_screen(I):\n","  I=np.dot(I[..., :3], [0.2989, 0.5870, 0.1140])\n","  #pdb.set_trace()\n","  return I.astype(np.float).ravel()\n","\n","\n","\n","agent = Agent(ALPHA = 0.0005, input_dims=210*160, GAMMA = 0.99, n_actions = 9,\n","            layer1_size = 64, layer2_size = 64)\n","\n","env = gym.make('Enduro-v0')\n","score_history = []\n","\n","n_episodes = 2000\n","for i in range(n_episodes):\n","    done = False\n","    score = 0\n","    observation = pong_preprocess_screen(env.reset())\n","\n","    while (not done):\n","        #pdb.set_trace()\n","        action = agent.choose_action(observation)\n","        observation_, reward, done, info = env.step(action)\n","        observation_=pong_preprocess_screen(observation_)\n","        agent.store_observation(observation, action, reward)\n","        observation=observation_\n","        score += reward\n","    score_history.append(score)\n","\n","    cost = agent.learn()\n","\n","    #pdb.set_trace()\n","    agent.probabilities = np.reshape(agent.probabilities, (9,-1))\n","    #plt.plot(agent.probabilities.sum(axis = 1))\n","    #plt.show()\n","    agent.probabilities=np.zeros(9)\n","    print('Episode {}, score {}, average_score {}, cost {}'.format(i, score, np.mean(score_history[-100:]), int(cost)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"],"name":"stderr"},{"output_type":"stream","text":["Episode 0, score 0.0, average_score 0.0, cost 0\n","Episode 1, score 0.0, average_score 0.0, cost 0\n","Episode 2, score 0.0, average_score 0.0, cost 21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IFR7ATsdIvDh","colab_type":"code","colab":{}},"source":["a=np.zeros([1,9])\n","b = np.ones([1,9])\n","c = np.concatenate((a,b), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"41ftP_gCJ_b0","colab_type":"code","colab":{}},"source":["a=np.array([[1,2,3], [1,2,3]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EaWQADH2KLPS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"becfeb9e-6e58-4776-b436-6573c5a79ccc","executionInfo":{"status":"ok","timestamp":1590013158273,"user_tz":-120,"elapsed":505,"user":{"displayName":"Juan Álvarez","photoUrl":"","userId":"06378331534551784227"}}},"source":["a.shape"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 9)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"JfUmNdjXKnkT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}